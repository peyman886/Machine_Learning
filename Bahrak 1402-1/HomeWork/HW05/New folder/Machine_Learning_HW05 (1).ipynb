{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<font size='+1' color=red>**Attention:**</font> Data cleaning and other parts of preprocessing of data which we covered in the first assignment, is not neccesary all the time but you may need some of them according to task at hand. So we don't explicitly mention them each time. This is your job to figure out when to apply them.\n",
        "\n",
        "<font size='+1' color=red>**Attention 2:**</font> For your implementations always use `random_state=42` so your code would be reproducible."
      ],
      "metadata": {
        "id": "xvYT4kv1TQpA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color='#D61E85' size='+3'>**Q1:**</font> <font size='+2'> **PCA for Classification** </font>\n",
        "\n",
        "In this question we want to work with the Fashion-MNIST dataset. Fashion-MNIST is a dataset comprising of $28 \\times 28$ grayscale images of $70,000$ fashion products from $10$ categories, with $7,000$ images per category. The training set has $60,000$ images and the test set has $10,000$ images. <br>\n",
        "<font color=red>**Note:**</font> You can download it from any source you want. <br>\n",
        "<font color=red>**Note:**</font> Take first $60,000$ instances of it as the train and the $10,000$ remaining instances as the test set.\n",
        "\n",
        "Using explained varinace ratio and considering a threshold like $95\\%$ you probably know how to choose the right number of dimensions to perform PCA. But, when you are using dimensionality reduction as a preprocessing step for a supervised learning task, it is important to consider the impact of the optimal number of dimensions on the overall performance of the model. Consider the classification task using the dataset at hand. Try to find the best number of components for the PCA with respect to the task. You should use the `RandomForestClassifier`, `KNeighborsClassifier`, `DecisionTreeClassifier`, and `AdaBoostClassifier`. Compare your results (number of dimensions, accuracy, precision, recall, f1-score, and confusion matrix) and explain why the number of dimensions for different models are different. Don't forget to analyze your results. [Hint: you should try to make a pipeline and try to tune the hyperparameters of PCA and your model adjointly.]\n",
        "\n",
        "At the end, perform the hyperparameter tuning but this time without considering the PCA preprocessing step. Compare your results with previous ones."
      ],
      "metadata": {
        "id": "PvVpPcQSYpJv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color='#8FCF26' size='+2'>**A1:**</font>"
      ],
      "metadata": {
        "id": "8rOUtFP1DHAy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Import Necessary Libraries\n"
      ],
      "metadata": {
        "id": "yuEirALGZoia"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "# from scipy.stats import randint"
      ],
      "metadata": {
        "id": "Ryc62iG4ZfJ9"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Load the Dataset"
      ],
      "metadata": {
        "id": "YeFCAEXPZ0BV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "fashion_mnist = fetch_openml('Fashion-MNIST', version=1)\n",
        "X = fashion_mnist.data\n",
        "y = fashion_mnist.target\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = X[:60000], X[60000:], y[:60000], y[60000:]"
      ],
      "metadata": {
        "id": "g1tGNFcUWfVJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a511a70-b2f3-4f0a-c75d-8005237b6bb3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/datasets/_openml.py:968: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
            "  warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Define the Pipelines and Parameters for Grid Search"
      ],
      "metadata": {
        "id": "1aB3QaZJZ4yb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipelines = {\n",
        "    'rf': Pipeline([('pca', PCA()), ('classifier', RandomForestClassifier(random_state=42))]),\n",
        "    'knn': Pipeline([('pca', PCA()), ('classifier', KNeighborsClassifier())]),\n",
        "    'dt': Pipeline([('pca', PCA()), ('classifier', DecisionTreeClassifier(random_state=42))]),\n",
        "    'ada': Pipeline([('pca', PCA()), ('classifier', AdaBoostClassifier(random_state=42))])\n",
        "}\n",
        "\n",
        "param_grid = {\n",
        "    'rf': {'pca__n_components': [0.85, 0.90, 0.95], 'classifier__n_estimators': [100, 200]},\n",
        "    'knn': {'pca__n_components': [0.85, 0.90, 0.95], 'classifier__n_neighbors': [3, 5, 7]},\n",
        "    'dt': {'pca__n_components': [0.85, 0.90, 0.95], 'classifier__max_depth': [10, 20, 30]},\n",
        "    'ada': {'pca__n_components': [0.85, 0.90, 0.95], 'classifier__n_estimators': [50, 100]}\n",
        "}\n"
      ],
      "metadata": {
        "id": "Vaj9v_ZVZlIH"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Train and Evaluate Each Model"
      ],
      "metadata": {
        "id": "8BZ6F525Z-zB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = {}\n",
        "for name, pipeline in pipelines.items():\n",
        "    # grid_search = GridSearchCV(pipeline, param_grid[name], cv=5, scoring='accuracy')\n",
        "    grid_search = RandomizedSearchCV(pipeline, param_grid[name], n_iter=5, cv=5, verbose=1, n_jobs=-1)\n",
        "    grid_search.fit(X_train, y_train)\n",
        "\n",
        "    best_model = grid_search.best_estimator_\n",
        "    predictions = best_model.predict(X_test)\n",
        "\n",
        "    results[name] = {\n",
        "        'Best Parameters': grid_search.best_params_,\n",
        "        'Number of PCA Components'\n",
        "        'Accuracy': accuracy_score(y_test, predictions),\n",
        "        'Precision': precision_score(y_test, predictions, average='macro'),\n",
        "        'Recall': recall_score(y_test, predictions, average='macro'),\n",
        "        'F1 Score': f1_score(y_test, predictions, average='macro'),\n",
        "        'Confusion Matrix': confusion_matrix(y_test, predictions)\n",
        "    }\n",
        "\n",
        "# Display results\n",
        "for model, metrics in results.items():\n",
        "    print(f\"Model: {model}\")\n",
        "    for metric, value in metrics.items():\n",
        "        print(f\"{metric}: {value}\")\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "id": "d4iNu_LTaEcQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Analysis"
      ],
      "metadata": {
        "id": "z-9osEWuYd-O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### RandomForestClassifier (rf)\n",
        "- **PCA Components**: 0.9\n",
        "- **Number of PCA Components**: 84\n",
        "- **Accuracy**: 0.8624\n",
        "- **Precision**: 0.8609\n",
        "- **Recall**: 0.8624\n",
        "- **F1 Score**: 0.8608\n",
        "\n",
        "**Analysis**: RandomForest performed well with a PCA component setting of 0.9. This indicates that capturing 90% of the variance in the data is sufficient for a complex ensemble method like RandomForest, which can handle a higher dimensional space effectively. The high accuracy and balanced precision-recall suggest that RandomForest is able to classify most classes accurately.\n",
        "\n",
        "#### KNeighborsClassifier (knn)\n",
        "- **PCA Components**: 0.95\n",
        "- **Accuracy**: 0.8623\n",
        "- **Precision**: 0.8631\n",
        "- **Recall**: 0.8623\n",
        "- **F1 Score**: 0.8616\n",
        "\n",
        "**Analysis**: KNN needed slightly more features (95% variance) to achieve similar performance to RandomForest. This is understandable as KNN relies heavily on the feature space for making decisions based on the nearest neighbors. The slight increase in the number of dimensions may provide more distinctiveness between different classes for KNN.\n",
        "\n",
        "#### DecisionTreeClassifier (dt)\n",
        "- **PCA Components**: 0.85\n",
        "- **Accuracy**: 0.7753\n",
        "- **Precision**: 0.7770\n",
        "- **Recall**: 0.7753\n",
        "- **F1 Score**: 0.7760\n",
        "\n",
        "**Analysis**: The DecisionTreeClassifier performed best with 85% variance, which is lower than the others. This might be because decision trees can overfit with too many features. A reduced feature set can sometimes help in preventing the model from fitting to noise in the data.\n",
        "\n",
        "#### AdaBoostClassifier (ada)\n",
        "- **PCA Components**: 0.95\n",
        "- **Accuracy**: 0.5746\n",
        "- **Precision**: 0.5769\n",
        "- **Recall**: 0.5746\n",
        "- **F1 Score**: 0.5626\n",
        "\n",
        "**Analysis**: AdaBoost with 95% variance components had the lowest performance among the models. AdaBoost is sensitive to noisy data and outliers, and the higher dimensional space might be introducing more complexity than the model can handle effectively.\n",
        "\n",
        "\n",
        "#### **Overall Observation**\n",
        "Different models require a different number of PCA components because each model has a unique way of handling and interpreting features. Models like RandomForest and KNN can benefit from a higher dimensional space as they can capture more complex patterns, while simpler models like DecisionTrees might perform better with fewer dimensions to prevent overfitting.\n",
        "\n",
        "The varying performance across models also highlights the importance of considering both the model's nature and the feature space's dimensionality when performing tasks like classification. The results also demonstrate the utility of PCA in reducing dimensionality while preserving enough information for effective model training and prediction."
      ],
      "metadata": {
        "id": "3CeQ7wRXYUDl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. tuning without considering the PCA preprocessing step"
      ],
      "metadata": {
        "id": "AjVeOqHGaB8O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6.1 Code"
      ],
      "metadata": {
        "id": "zJlyGko5qO3J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classifiers = {\n",
        "    'rf': RandomForestClassifier(random_state=42),\n",
        "    'knn': KNeighborsClassifier(),\n",
        "    'dt': DecisionTreeClassifier(random_state=42),\n",
        "    'ada': AdaBoostClassifier(random_state=42)\n",
        "}\n",
        "\n",
        "param_grid_no_pca = {\n",
        "    'rf': {'n_estimators': [100, 200]},\n",
        "    'knn': {'n_neighbors': [3, 5, 7]},\n",
        "    'dt': {'max_depth': [10, 20, 30]},\n",
        "    'ada': {'n_estimators': [50, 100]}\n",
        "}"
      ],
      "metadata": {
        "id": "fMWlc_UlaTXN"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_no_pca = {}\n",
        "for name, classifier in classifiers.items():\n",
        "    grid_search_no_pca = RandomizedSearchCV(classifier, param_grid_no_pca[name],\n",
        "                                            scoring='accuracy', n_iter=5, cv=5, verbose=1, n_jobs=-1)\n",
        "    grid_search_no_pca.fit(X_train, y_train)\n",
        "\n",
        "    best_model_no_pca = grid_search_no_pca.best_estimator_\n",
        "    predictions_no_pca = best_model_no_pca.predict(X_test)\n",
        "\n",
        "    results_no_pca[name] = {\n",
        "        'Best Parameters': grid_search_no_pca.best_params_,\n",
        "        'Accuracy': accuracy_score(y_test, predictions_no_pca),\n",
        "        'Precision': precision_score(y_test, predictions_no_pca, average='macro'),\n",
        "        'Recall': recall_score(y_test, predictions_no_pca, average='macro'),\n",
        "        'F1 Score': f1_score(y_test, predictions_no_pca, average='macro'),\n",
        "        'Confusion Matrix': confusion_matrix(y_test, predictions_no_pca)\n",
        "    }\n",
        "\n",
        "# Display results\n",
        "for model, metrics in results_no_pca.items():\n",
        "    print(f\"Model: {model}\")\n",
        "    for metric, value in metrics.items():\n",
        "        print(f\"{metric}: {value}\")\n",
        "    print(\"\\n\")\n"
      ],
      "metadata": {
        "id": "a992MUgPaXpO",
        "outputId": "cb2d40c5-414c-4232-83ed-66eceecd012e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:305: UserWarning: The total space of parameters 2 is smaller than n_iter=5. Running 2 iterations. For exhaustive searches, use GridSearchCV.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:305: UserWarning: The total space of parameters 3 is smaller than n_iter=5. Running 3 iterations. For exhaustive searches, use GridSearchCV.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:305: UserWarning: The total space of parameters 3 is smaller than n_iter=5. Running 3 iterations. For exhaustive searches, use GridSearchCV.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:305: UserWarning: The total space of parameters 2 is smaller than n_iter=5. Running 2 iterations. For exhaustive searches, use GridSearchCV.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
            "Model: rf\n",
            "Best Parameters: {'n_estimators': 200}\n",
            "Accuracy: 0.8775\n",
            "Precision: 0.8765315291845133\n",
            "Recall: 0.8775000000000001\n",
            "F1 Score: 0.8761952314481413\n",
            "Confusion Matrix: [[860   0  11  27   4   1  84   0  13   0]\n",
            " [  3 965   3  21   2   0   5   0   1   0]\n",
            " [ 11   0 800   9 117   0  60   0   3   0]\n",
            " [ 17   3  10 904  34   0  30   0   2   0]\n",
            " [  0   0  91  33 823   0  51   0   2   0]\n",
            " [  0   0   0   1   0 959   0  29   2   9]\n",
            " [151   1 123  29  88   0 592   0  16   0]\n",
            " [  0   0   0   0   0  11   0 953   0  36]\n",
            " [  1   1   4   4   5   2   5   3 974   1]\n",
            " [  0   0   0   0   0   9   1  43   2 945]]\n",
            "\n",
            "\n",
            "Model: knn\n",
            "Best Parameters: {'n_neighbors': 5}\n",
            "Accuracy: 0.8554\n",
            "Precision: 0.8578152450755354\n",
            "Recall: 0.8554\n",
            "F1 Score: 0.8546439722018905\n",
            "Confusion Matrix: [[855   1  17  16   3   1 100   1   6   0]\n",
            " [  8 968   4  12   4   0   3   0   1   0]\n",
            " [ 24   2 819  11  75   0  69   0   0   0]\n",
            " [ 41   8  15 860  39   0  34   0   3   0]\n",
            " [  2   1 126  26 773   0  71   0   1   0]\n",
            " [  1   0   0   0   0 822   5  96   1  75]\n",
            " [176   1 132  23  80   0 575   0  13   0]\n",
            " [  0   0   0   0   0   3   0 961   0  36]\n",
            " [  2   0  10   4   7   0  16   7 953   1]\n",
            " [  0   0   0   0   0   2   1  29   0 968]]\n",
            "\n",
            "\n",
            "Model: dt\n",
            "Best Parameters: {'max_depth': 10}\n",
            "Accuracy: 0.8009\n",
            "Precision: 0.8046192725818088\n",
            "Recall: 0.8009000000000001\n",
            "F1 Score: 0.801520058199\n",
            "Confusion Matrix: [[774   3  25  40  12   0 132   1   9   4]\n",
            " [ 13 924   4  40   7   0  10   0   1   1]\n",
            " [ 13   1 665   8 242   0  62   0   8   1]\n",
            " [ 32  11  26 811  62   1  50   0   6   1]\n",
            " [  4   0 124  39 734   0  96   0   3   0]\n",
            " [  2   4   0   1   1 864   0  70  21  37]\n",
            " [135   4 164  38 140   2 499   0  15   3]\n",
            " [  0   0   0   0   0  27   0 928   6  39]\n",
            " [  4   2  17  10  20   7  27   9 902   2]\n",
            " [  1   0   1   0   0  21   1  65   3 908]]\n",
            "\n",
            "\n",
            "Model: ada\n",
            "Best Parameters: {'n_estimators': 50}\n",
            "Accuracy: 0.5425\n",
            "Precision: 0.5612784019025104\n",
            "Recall: 0.5425\n",
            "F1 Score: 0.5182906120607471\n",
            "Confusion Matrix: [[ 47  21 302 296  26   1 297   0   8   2]\n",
            " [  3 804  37 143   8   1   4   0   0   0]\n",
            " [  9   5 604  35 304   0  34   0   9   0]\n",
            " [  7  90 116 659 115   0  10   0   3   0]\n",
            " [  7   5 342  41 595   0   7   0   3   0]\n",
            " [  0   0   0   0   1 557   0 313  47  82]\n",
            " [ 19  10 483 139 237   0  93   0  19   0]\n",
            " [  0   0   0   0   0  83   0 901   4  12]\n",
            " [ 31   1  12   8   9  20 112  53 751   3]\n",
            " [  0   0   0   1   0 111   1 464   9 414]]\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6.2 Analysis"
      ],
      "metadata": {
        "id": "5QrKVIFiqW5N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### RandomForestClassifier (rf)\n",
        "- **With PCA**: Accuracy = 0.8624, PCA Components = 0.9, Number of PCA Components = 84\n",
        "- **Without PCA**: Accuracy = 0.8775\n",
        "- **Observation**: RandomForest showed a slight improvement in accuracy without PCA. This suggests that the original feature set contains useful information that gets lost during dimensionality reduction.\n",
        "\n",
        "##### KNeighborsClassifier (knn)\n",
        "- **With PCA**: Accuracy = 0.8623, PCA Components = 0.95, Number of PCA Components = 187\n",
        "- **Without PCA**: Accuracy = 0.8554\n",
        "- **Observation**: KNN performed slightly better with PCA. This indicates that reducing the dimensions helped KNN to process the data more effectively, possibly by removing noise or irrelevant features.\n",
        "\n",
        "##### DecisionTreeClassifier (dt)\n",
        "- **With PCA**: Accuracy = 0.7753, PCA Components = 0.85, PCA Number of PCA Components = 43\n",
        "- **Without PCA**: Accuracy = 0.8009\n",
        "- **Observation**: DecisionTreeClassifier showed an improvement without PCA. This could be because decision trees can handle a large number of features well and can benefit from more detailed information in the dataset.\n",
        "\n",
        "##### AdaBoostClassifier (ada)\n",
        "- **With PCA**: Accuracy = 0.5746, PCA Components = 0.95, Number of PCA Components = 187\n",
        "- **Without PCA**: Accuracy = 0.5425\n",
        "- **Observation**: AdaBoost performed poorly in both scenarios, but it was slightly better with PCA. This might indicate that AdaBoost is sensitive to the number of features, and dimensionality reduction helps to some extent.\n",
        "\n",
        "##### General Observations\n",
        "1. **RandomForest**: The improvement without PCA suggests that RandomForest can effectively handle a high-dimensional feature space and extract useful information from it.\n",
        "2. **KNN**: The slight decline in performance without PCA indicates that KNN benefits from a reduced feature space, likely because it reduces the computational complexity and potential noise in the data.\n",
        "3. **DecisionTree**: The improvement without PCA shows that decision trees can effectively handle complex feature interactions in higher dimensions without overfitting.\n",
        "4. **AdaBoost**: The relatively poor performance in both scenarios suggests that AdaBoost might not be the best choice for this particular dataset, regardless of dimensionality.\n",
        "\n",
        "##### Conclusion\n",
        "The comparison reveals that the usefulness of PCA depends on the nature of the classifier and the dataset. While PCA can help in reducing overfitting and computational complexity for some models (like KNN), others (like RandomForest and DecisionTree) might perform better with the complete feature set, leveraging the full information available in the data. This highlights the importance of understanding both the dataset characteristics and the model's strengths and weaknesses when deciding on preprocessing steps like PCA."
      ],
      "metadata": {
        "id": "FxkWZ_7dqItZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color='#D61E85' size='+3'>**Q2:**</font> <font size='+2'> **Randomized PCA** </font>\n",
        "\n",
        "In this question we want to check the time complexity of finding an approximation of the first $d$ principal components. Also, we want to see how is the performance of it in compare to the original PCA. In order to make this happen there is a stochastic algorithm called *randomized PCA* which has a faster procedure to find the first $d$ principal components.\n",
        "\n",
        "By default, the `svd_solver` parameter of PCA in Scikit-learn is set to `\"auto\"`. It means that it automatically determine to use `\"full\"` or `\"randomized\"` to find the principal components. Base on our text book:\n",
        "\n",
        "\"Scikit-learn uses the randomized PCA algorithm if $max(m, n) > 500$ and `n_components` is an integer smaller than $80\\%$ of $min(m,n)$, or else it uses the full SVD approach\"\n",
        "\n",
        "<font size='+1'>**(a)**</font> For previous question you found $d$ components for each one of the classifiers.This time try to perform PCA without considering the classifier and just by determining the number of components. For the `svd_solver`, this time use both `\"full\"` and `\"randomized\"` arguments separately and compare the results of them. Also compare the running time of `\"full\"` and `\"randomized\"` for each one of the classifiers. Explain your observations. You should perform following steps one-by-one for each classifier:\n",
        "\n",
        "*   Perform PCA for both `\"full\"` and `\"randomized\"`. Repoert the time of performing them.\n",
        "\n",
        "*   For both cases fit the new training data (after dimensionality reduction) to the classifier. Then report the results (Accuracy, F1-score, ...) on test set.\n",
        "\n",
        "<font size='+1'>**(b)**</font> This time consider the $d=10$ and compare the running times for both `\"full\"` and `\"randomized\"` arguments. Explain your observations.\n",
        "\n",
        "<font size='+1'>**(c)**</font> There is something called Incremental PCA (IPCA), explain what is it and in what situations it is useful?\n",
        "\n",
        "<font size='+1'>**(d)**</font> Consider the number of batches equal to $200$ and perform the IPCA."
      ],
      "metadata": {
        "id": "TQc4j8JMWdx1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color='#8FCF26' size='+2'>**A2:**</font>"
      ],
      "metadata": {
        "id": "Z9C3GhsyDVib"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time"
      ],
      "metadata": {
        "id": "Iob5574QDTvH"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_components_rf = 84 # Number of components for RandomForest with PCA Components = 0.9\n",
        "n_components_knn = 187 # Number of components for KNN with PCA Components = 0.95\n",
        "n_components_dt = 43 # Number of components for DecisionTree with PCA Components = 0.85\n",
        "n_components_ada = 187 # Number of components for AdaBoost with PCA Components = 0.95\n",
        "\n",
        "classifiers = {\n",
        "    'rf': RandomForestClassifier(n_estimators=200, random_state=42),\n",
        "    'knn': KNeighborsClassifier(n_neighbors=5),\n",
        "    'dt': DecisionTreeClassifier(max_depth=20, random_state=42),\n",
        "    'ada': AdaBoostClassifier(n_estimators=50, random_state=42)\n",
        "}\n",
        "\n",
        "components_dict = {\n",
        "    'rf': n_components_rf,\n",
        "    'knn': n_components_knn,\n",
        "    'dt': n_components_dt,\n",
        "    'ada': n_components_ada\n",
        "}\n"
      ],
      "metadata": {
        "id": "ZQTC7FT8t5lE"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pca_times = {}\n",
        "\n",
        "for clf_name, clf in classifiers.items():\n",
        "    n_components = components_dict[clf_name]\n",
        "\n",
        "    # Full PCA\n",
        "    start = time.time()\n",
        "    pca_full = PCA(n_components=n_components, svd_solver='full').fit(X_train)\n",
        "    X_train_pca_full = pca_full.transform(X_train)\n",
        "    end = time.time()\n",
        "    pca_times[clf_name + \"_full\"] = end - start\n",
        "\n",
        "    # Randomized PCA\n",
        "    start = time.time()\n",
        "    pca_randomized = PCA(n_components=n_components, svd_solver='randomized').fit(X_train)\n",
        "    X_train_pca_randomized = pca_randomized.transform(X_train)\n",
        "    end = time.time()\n",
        "    pca_times[clf_name + \"_randomized\"] = end - start\n",
        "\n",
        "# Display PCA times\n",
        "for pca_type, time_taken in pca_times.items():\n",
        "    print(f\"{pca_type}: {time_taken} seconds\")\n"
      ],
      "metadata": {
        "id": "gTbAdgRkt33k",
        "outputId": "5778e99d-c559-4aa9-c147-e6326f7e0d9b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rf_full: 18.32205891609192 seconds\n",
            "rf_randomized: 8.859454870223999 seconds\n",
            "knn_full: 19.524474143981934 seconds\n",
            "knn_randomized: 17.51714563369751 seconds\n",
            "dt_full: 24.481194257736206 seconds\n",
            "dt_randomized: 7.668455123901367 seconds\n",
            "ada_full: 16.292694091796875 seconds\n",
            "ada_randomized: 13.990559816360474 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color='#D61E85' size='+3'>**Q3:**</font> <font size='+2'> **Locally Linear Embedding** </font>\n",
        "\n",
        "Locally linear embedding (LLE), is a nonlinear dimensionality reduction algorithm which is categorized as a manifold learning technique.\n",
        "\n",
        "<font size='+1'>**(a)**</font> At first, try to explain how it works by mentioning its optimization objectives.\n",
        "\n",
        "<font size='+1'>**(b)**</font> Now, it's time to implement it and trying to perform your implementation on a swiss roll to see what happens after unrolling. (try to plot your results)\n",
        "The code below make you a swiss roll with $1000$ samples.\n",
        "\n",
        "<font size='+1'>**(c)**</font> Finally use the LLE implementation provided by Scikit-learn to check the results of your implementation. (plot your results)"
      ],
      "metadata": {
        "id": "JfezDIMqW_OL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color='#8FCF26' size='+2'>**A3:**</font> Your explanations"
      ],
      "metadata": {
        "id": "lhJB7JkmEvTG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_swiss_roll\n",
        "\n",
        "X_swiss, t = make_swiss_roll(n_samples=1000, noise=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "ot2rwJlSW-Yx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code"
      ],
      "metadata": {
        "id": "3gcIzPzZEJm5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color='#D61E85' size='+3'>**Q4:**</font> <font size='+2'> **t-SNE vs. UMAP ‚ÄéÔ∏è‚Äçüî•** </font>\n",
        "\n",
        "In this question we need the first $5000$ images of Fashion-MNIST dataset. We want to reduce the dimension of these samples down to 2 so we can plot them. Here, we use t-SNE and UMAP to perform these reductions. You can use scatterplot with 10 different colors to demonstrate the class of each instance. After visulaization try to analyze your results and compare them with each other. Is there any pattern in these visualizations?"
      ],
      "metadata": {
        "id": "MlITuQBUiohI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color='#8FCF26' size='+2'>**A4:**</font> Your explanations"
      ],
      "metadata": {
        "id": "aOpTYSRME1jA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code"
      ],
      "metadata": {
        "id": "aVh8hnxZvl3H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color='#D61E85' size='+3'>**Q5:**</font> <font size='+2'> **Iris** </font>\n",
        "\n",
        "You will take a shortcut and load the Iris dataset from Scikit-learn‚Äôs datasets module. Furthermore, you will only select two features, sepal width and petal length, to make the classification task more challenging for illustration purposes"
      ],
      "metadata": {
        "id": "Fd_-f0gjp5bX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "-4_nATDzrl28"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Your Code"
      ],
      "metadata": {
        "id": "yoODjVwHrv_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "split the Iris examples into 50 percent training and 50 percent test data:"
      ],
      "metadata": {
        "id": "srxliDlC4lFZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your Code"
      ],
      "metadata": {
        "id": "eRWmkBhYsizR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the training dataset, you now will train three different classifiers:\n",
        "\n",
        "- Logistic regression classifier\n",
        "\n",
        "- Decision tree classifier\n",
        "\n",
        "- k-nearest neighbors classifier\n",
        "\n",
        "you will then evaluate the model performance of each classifier via 10-fold cross-validation on the training dataset before combining them into an ensemble classifier:"
      ],
      "metadata": {
        "id": "0Le3UZmU4oh7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your Code"
      ],
      "metadata": {
        "id": "iYd5PyN9rxsn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color='#D61E85' size='+3'>**Q6:**</font> <font size='+2'> **Carseats** </font>\n",
        "\n",
        "#### Ensemble Learning\n",
        "\n",
        "Ensemble learning is a machine learning technique that involves combining the predictions of multiple models to improve the overall performance and accuracy of a system. Instead of relying on a single model to make predictions, ensemble methods use a group of models and aggregate their predictions to achieve better results than any individual model could achieve on its own.\n",
        "\n",
        "The basic idea behind ensemble learning is that by combining the strengths of different models, it is possible to mitigate the weaknesses of each individual model. Ensemble methods are often used to enhance predictive accuracy, reduce overfitting, and improve the robustness of the model.\n",
        "\n",
        "There are several popular ensemble learning techniques, including:\n",
        "\n",
        "**Bagging (Bootstrap Aggregating):** This method involves training multiple instances of the same learning algorithm on different subsets of the training data, typically created by random sampling with replacement. The predictions of these models are then averaged or voted upon to make the final prediction.\n",
        "\n",
        "**Boosting:** Boosting focuses on training a sequence of weak learners, where each subsequent model corrects the errors of its predecessor. Popular boosting algorithms include AdaBoost (Adaptive Boosting) and Gradient Boosting.\n",
        "\n",
        "**Random Forest:** Random Forest is an ensemble method based on bagging. It constructs multiple decision trees during training and combines their predictions through averaging or voting. Each tree in the forest is trained on a random subset of the features.\n",
        "\n",
        "Stacking: Stacking involves training multiple diverse models and using another model (meta-model or blender) to combine their predictions. The predictions of individual models serve as input features for the meta-model.\n",
        "\n",
        "Ensemble learning is a powerful technique that is widely used in various machine learning applications. It is particularly effective when dealing with complex and diverse datasets, as well as when individual models may have different strengths and weaknesses."
      ],
      "metadata": {
        "id": "N0o_08_dpydx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are going to work with **Carseats** dataset. We want to predict the sales using regression trees and related approaches, treating the response as a quantitative variable."
      ],
      "metadata": {
        "id": "piAQAOuOpijJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Load Dataset"
      ],
      "metadata": {
        "id": "1mzLGdm4pwKg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor, export_graphviz"
      ],
      "metadata": {
        "id": "oym5JLQdcy11"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Do preprocess\n",
        "- Split the data set into a training set and a test set."
      ],
      "metadata": {
        "id": "44HOtayap3Su"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your Code:"
      ],
      "metadata": {
        "id": "xFVYdiZeqRoh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Fit a regression tree to the training set. Plot the tree, and interpret\n",
        "the results. What test MSE do you obtain?"
      ],
      "metadata": {
        "id": "ReN9KAYBqUqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code"
      ],
      "metadata": {
        "id": "dromxeOMq3Zq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Use the bagging approach in order to analyze this data. What test MSE do you obtain? which variables are most important. visualize them"
      ],
      "metadata": {
        "id": "LAkQPtSfr1JH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code"
      ],
      "metadata": {
        "id": "MWOPM20ErVMS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Use random forests to analyze this data. What test MSE do you obtain?  which variables are most important. Describe the effect of m, the number of variables considered at each split, on the error rate obtained."
      ],
      "metadata": {
        "id": "HgADUcGXsSmc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code"
      ],
      "metadata": {
        "id": "x7ajvS2BrXxK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}